# -*- coding: utf-8 -*-
"""HoneypotModelcomparisions.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1iiSu0Inb6H3V0eA54JFGf2vPzvKIiyxp
"""

!pip install pandas numpy matplotlib seaborn scikit-learn plotly ipywidgets
!pip install requests beautifulsoup4 geoip2 folium

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.svm import SVC
from sklearn.naive_bayes import GaussianNB
from sklearn.neighbors import KNeighborsClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, roc_auc_score
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.decomposition import PCA
import warnings
warnings.filterwarnings('ignore')

print("✅ All libraries imported successfully!")
print("🔥 Honeypot Threat Prediction ML System Ready!")

# ================================
# CELL 3: Generate Synthetic Honeypot Data
# ================================
def generate_honeypot_data(n_samples=10000):
    """
    Generate synthetic honeypot attack data similar to T-Pot logs
    """
    np.random.seed(42)

    # Attack types based on honeypot services
    attack_types = ['SSH_BRUTEFORCE', 'WEB_ATTACK', 'FTP_ATTACK', 'SMTP_ATTACK',
                   'MYSQL_ATTACK', 'TELNET_ATTACK', 'NORMAL_TRAFFIC']

    # Geographic regions for IP simulation
    countries = ['CN', 'US', 'RU', 'BR', 'IN', 'TR', 'VN', 'KR', 'DE', 'FR']

    data = []
    for i in range(n_samples):
        # Simulate IP addresses
        ip_prefix = np.random.choice([1, 10, 172, 192, 203, 117, 223])
        source_ip = f"{ip_prefix}.{np.random.randint(1,255)}.{np.random.randint(1,255)}.{np.random.randint(1,255)}"

        # Attack characteristics
        attack_type = np.random.choice(attack_types, p=[0.25, 0.20, 0.15, 0.10, 0.10, 0.10, 0.10])

        # Features based on attack type
        if attack_type == 'SSH_BRUTEFORCE':
            connection_count = np.random.randint(50, 500)
            payload_size = np.random.randint(50, 200)
            session_duration = np.random.randint(1, 30)
            port = 22
            failed_attempts = np.random.randint(10, 100)
        elif attack_type == 'WEB_ATTACK':
            connection_count = np.random.randint(10, 100)
            payload_size = np.random.randint(200, 2000)
            session_duration = np.random.randint(5, 60)
            port = np.random.choice([80, 443, 8080])
            failed_attempts = np.random.randint(0, 20)
        elif attack_type == 'NORMAL_TRAFFIC':
            connection_count = np.random.randint(1, 10)
            payload_size = np.random.randint(50, 500)
            session_duration = np.random.randint(1, 300)
            port = np.random.choice([80, 443, 22, 21, 25])
            failed_attempts = 0
        else:
            connection_count = np.random.randint(5, 50)
            payload_size = np.random.randint(100, 1000)
            session_duration = np.random.randint(5, 120)
            port = np.random.choice([21, 25, 3306, 23])
            failed_attempts = np.random.randint(0, 50)

        # Additional features
        country = np.random.choice(countries)
        hour_of_day = np.random.randint(0, 24)
        day_of_week = np.random.randint(0, 7)

        # Threat level (target variable)
        if attack_type == 'NORMAL_TRAFFIC':
            threat_level = 0  # Normal
        elif attack_type in ['SSH_BRUTEFORCE', 'WEB_ATTACK']:
            threat_level = np.random.choice([2, 3], p=[0.7, 0.3])  # High/Critical
        else:
            threat_level = np.random.choice([1, 2], p=[0.6, 0.4])  # Medium/High

        data.append({
            'source_ip': source_ip,
            'attack_type': attack_type,
            'connection_count': connection_count,
            'payload_size': payload_size,
            'session_duration': session_duration,
            'destination_port': port,
            'failed_login_attempts': failed_attempts,
            'source_country': country,
            'hour_of_day': hour_of_day,
            'day_of_week': day_of_week,
            'packets_sent': np.random.randint(1, 1000),
            'bytes_transferred': payload_size * connection_count,
            'unique_protocols': np.random.randint(1, 5),
            'threat_level': threat_level
        })

    return pd.DataFrame(data)

# Generate the dataset
df = generate_honeypot_data(15000)
print("🎯 Honeypot Dataset Generated Successfully!")
print(f"📊 Dataset Shape: {df.shape}")
print(f"🔍 Columns: {list(df.columns)}")
print("\n📈 First 5 rows:")
print(df.head())

# ================================
# CELL 4: Data Exploration & Visualization
# ================================
print("="*50)
print("📊 HONEYPOT DATA ANALYSIS")
print("="*50)

# Basic statistics
print("\n🔍 Dataset Info:")
print(df.info())
print(f"\n📊 Dataset Shape: {df.shape}")
print(f"🎯 Target Distribution:")
print(df['threat_level'].value_counts().sort_index())

# Threat level distribution
plt.figure(figsize=(15, 10))

# Subplot 1: Threat Level Distribution
plt.subplot(2, 3, 1)
threat_counts = df['threat_level'].value_counts().sort_index()
colors = ['green', 'orange', 'red', 'darkred']
plt.pie(threat_counts.values, labels=['Normal', 'Medium', 'High', 'Critical'],
        colors=colors, autopct='%1.1f%%', startangle=90)
plt.title('🎯 Threat Level Distribution')

# Subplot 2: Attack Types
plt.subplot(2, 3, 2)
attack_counts = df['attack_type'].value_counts()
plt.bar(range(len(attack_counts)), attack_counts.values, color='skyblue')
plt.xticks(range(len(attack_counts)), attack_counts.index, rotation=45, ha='right')
plt.title('🔥 Attack Types Distribution')
plt.ylabel('Count')

# Subplot 3: Top Source Countries
plt.subplot(2, 3, 3)
country_counts = df['source_country'].value_counts().head(10)
plt.bar(country_counts.index, country_counts.values, color='lightcoral')
plt.title('🌍 Top Source Countries')
plt.ylabel('Attack Count')

# Subplot 4: Connection Count vs Threat Level
plt.subplot(2, 3, 4)
for threat in sorted(df['threat_level'].unique()):
    data = df[df['threat_level'] == threat]['connection_count']
    plt.hist(data, alpha=0.7, bins=30, label=f'Threat {threat}')
plt.xlabel('Connection Count')
plt.ylabel('Frequency')
plt.title('📈 Connection Count by Threat Level')
plt.legend()

# Subplot 5: Attack Timeline
plt.subplot(2, 3, 5)
hourly_attacks = df.groupby('hour_of_day')['threat_level'].mean()
plt.plot(hourly_attacks.index, hourly_attacks.values, marker='o', color='red')
plt.xlabel('Hour of Day')
plt.ylabel('Average Threat Level')
plt.title('⏰ Attack Intensity by Hour')
plt.grid(True)

# Subplot 6: Port Analysis
plt.subplot(2, 3, 6)
port_threats = df.groupby('destination_port')['threat_level'].mean().sort_values(ascending=False).head(10)
plt.bar(range(len(port_threats)), port_threats.values, color='purple')
plt.xticks(range(len(port_threats)), port_threats.index, rotation=45)
plt.title('🔌 Most Targeted Ports')
plt.ylabel('Avg Threat Level')

plt.tight_layout()
plt.show()

# Correlation heatmap
plt.figure(figsize=(12, 8))
# Select only numeric columns for correlation
numeric_cols = df.select_dtypes(include=[np.number]).columns
correlation_matrix = df[numeric_cols].corr()
sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', center=0,
            square=True, linewidths=0.5)
plt.title('🔥 Feature Correlation Heatmap')
plt.show()

# ================================
# CELL 5: Feature Engineering & Preprocessing
# ================================
print("\n🔧 FEATURE ENGINEERING & PREPROCESSING")
print("="*50)

# Create feature engineering function
def feature_engineering(df):
    """
    Advanced feature engineering for honeypot data
    """
    df_processed = df.copy()

    # 1. IP-based features
    df_processed['ip_class'] = df_processed['source_ip'].apply(lambda x: x.split('.')[0])
    df_processed['is_private_ip'] = df_processed['source_ip'].apply(
        lambda x: x.startswith(('192.168', '10.', '172.'))
    ).astype(int)

    # 2. Behavioral features
    df_processed['attack_intensity'] = (
        df_processed['connection_count'] * df_processed['failed_login_attempts']
    ) / (df_processed['session_duration'] + 1)

    df_processed['payload_per_connection'] = (
        df_processed['payload_size'] / (df_processed['connection_count'] + 1)
    )

    df_processed['bytes_per_packet'] = (
        df_processed['bytes_transferred'] / (df_processed['packets_sent'] + 1)
    )

    # 3. Temporal features
    df_processed['is_weekend'] = (df_processed['day_of_week'] >= 5).astype(int)
    df_processed['is_night_time'] = (
        (df_processed['hour_of_day'] >= 22) | (df_processed['hour_of_day'] <= 6)
    ).astype(int)

    # 4. Risk scoring features
    high_risk_countries = ['CN', 'RU', 'TR', 'VN']
    df_processed['is_high_risk_country'] = (
        df_processed['source_country'].isin(high_risk_countries)
    ).astype(int)

    high_risk_ports = [22, 23, 21, 3306]
    df_processed['is_high_risk_port'] = (
        df_processed['destination_port'].isin(high_risk_ports)
    ).astype(int)

    # 5. Composite features
    df_processed['anomaly_score'] = (
        df_processed['connection_count'] +
        df_processed['failed_login_attempts'] +
        df_processed['is_high_risk_country'] * 10 +
        df_processed['is_high_risk_port'] * 5
    )

    return df_processed

# Apply feature engineering
df_engineered = feature_engineering(df)
print(f"✅ Feature engineering complete!")
print(f"📈 New features added: {len(df_engineered.columns) - len(df.columns)}")
print(f"📊 Total features: {len(df_engineered.columns)}")

# Prepare features for ML
def prepare_ml_features(df):
    """
    Prepare features for machine learning
    """
    # Encode categorical variables
    le_attack = LabelEncoder()
    le_country = LabelEncoder()
    le_ip_class = LabelEncoder()

    df_ml = df.copy()
    df_ml['attack_type_encoded'] = le_attack.fit_transform(df_ml['attack_type'])
    df_ml['country_encoded'] = le_country.fit_transform(df_ml['source_country'])
    df_ml['ip_class_encoded'] = le_ip_class.fit_transform(df_ml['ip_class'])

    # Select features for ML
    feature_columns = [
        'connection_count', 'payload_size', 'session_duration',
        'destination_port', 'failed_login_attempts', 'hour_of_day',
        'day_of_week', 'packets_sent', 'bytes_transferred', 'unique_protocols',
        'attack_type_encoded', 'country_encoded', 'ip_class_encoded',
        'is_private_ip', 'attack_intensity', 'payload_per_connection',
        'bytes_per_packet', 'is_weekend', 'is_night_time',
        'is_high_risk_country', 'is_high_risk_port', 'anomaly_score'
    ]

    X = df_ml[feature_columns]
    y = df_ml['threat_level']

    return X, y, feature_columns, (le_attack, le_country, le_ip_class)

X, y, feature_names, encoders = prepare_ml_features(df_engineered)
print(f"🎯 Features prepared: {X.shape}")
print(f"📊 Target distribution: {y.value_counts().to_dict()}")

# ================================
# CELL 6: Machine Learning Models Training
# ================================
print("\n🤖 MACHINE LEARNING MODELS TRAINING")
print("="*50)

# Split the data
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42, stratify=y
)

# Scale the features
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

print(f"📊 Training set: {X_train.shape}")
print(f"📊 Test set: {X_test.shape}")

# Define ML models
models = {
    'Random Forest': RandomForestClassifier(n_estimators=100, random_state=42),
    'Gradient Boosting': GradientBoostingClassifier(random_state=42),
    'Logistic Regression': LogisticRegression(random_state=42, max_iter=1000),
    'SVM': SVC(probability=True, random_state=42),
    'Naive Bayes': GaussianNB(),
    'K-Neighbors': KNeighborsClassifier(),
    'Decision Tree': DecisionTreeClassifier(random_state=42)
}

# Train and evaluate models
results = {}
model_objects = {}

for name, model in models.items():
    print(f"\n🔄 Training {name}...")

    # Use scaled data for distance-based algorithms
    if name in ['SVM', 'K-Neighbors', 'Logistic Regression']:
        model.fit(X_train_scaled, y_train)
        y_pred = model.predict(X_test_scaled)
        y_pred_proba = model.predict_proba(X_test_scaled)
    else:
        model.fit(X_train, y_train)
        y_pred = model.predict(X_test)
        y_pred_proba = model.predict_proba(X_test)

    # Calculate metrics
    accuracy = accuracy_score(y_test, y_pred)

    # Cross-validation
    if name in ['SVM', 'K-Neighbors', 'Logistic Regression']:
        cv_scores = cross_val_score(model, X_train_scaled, y_train, cv=5)
    else:
        cv_scores = cross_val_score(model, X_train, y_train, cv=5)

    results[name] = {
        'accuracy': accuracy,
        'cv_mean': cv_scores.mean(),
        'cv_std': cv_scores.std(),
        'predictions': y_pred,
        'probabilities': y_pred_proba
    }

    model_objects[name] = model

    print(f"✅ {name} - Accuracy: {accuracy:.4f}, CV: {cv_scores.mean():.4f} ± {cv_scores.std():.4f}")

# ================================
# CELL 7: Model Evaluation & Visualization
# ================================
print("\n📈 MODEL EVALUATION & RESULTS")
print("="*50)

# Create results DataFrame
results_df = pd.DataFrame({
    'Model': list(results.keys()),
    'Accuracy': [results[model]['accuracy'] for model in results.keys()],
    'CV Mean': [results[model]['cv_mean'] for model in results.keys()],
    'CV Std': [results[model]['cv_std'] for model in results.keys()]
})

results_df = results_df.sort_values('Accuracy', ascending=False)
print("🏆 MODEL PERFORMANCE RANKING:")
print(results_df.to_string(index=False))

# Visualization
fig, axes = plt.subplots(2, 2, figsize=(15, 12))

# 1. Model Comparison
axes[0, 0].bar(results_df['Model'], results_df['Accuracy'], color='skyblue')
axes[0, 0].set_title('🏆 Model Accuracy Comparison')
axes[0, 0].set_ylabel('Accuracy')
axes[0, 0].tick_params(axis='x', rotation=45)

# 2. Cross-validation scores
axes[0, 1].bar(results_df['Model'], results_df['CV Mean'],
               yerr=results_df['CV Std'], capsize=5, color='lightgreen')
axes[0, 1].set_title('📊 Cross-Validation Scores')
axes[0, 1].set_ylabel('CV Score')
axes[0, 1].tick_params(axis='x', rotation=45)

# 3. Best model confusion matrix
best_model_name = results_df.iloc[0]['Model']
best_predictions = results[best_model_name]['predictions']
cm = confusion_matrix(y_test, best_predictions)
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=axes[1, 0])
axes[1, 0].set_title(f'🎯 Confusion Matrix - {best_model_name}')
axes[1, 0].set_xlabel('Predicted')
axes[1, 0].set_ylabel('Actual')

# 4. Feature importance (for tree-based models)
if best_model_name in ['Random Forest', 'Gradient Boosting', 'Decision Tree']:
    feature_importance = model_objects[best_model_name].feature_importances_
    feature_df = pd.DataFrame({
        'feature': feature_names,
        'importance': feature_importance
    }).sort_values('importance', ascending=False).head(10)

    axes[1, 1].barh(feature_df['feature'], feature_df['importance'], color='orange')
    axes[1, 1].set_title(f'🔍 Top 10 Features - {best_model_name}')
    axes[1, 1].set_xlabel('Importance')
else:
    axes[1, 1].text(0.5, 0.5, 'Feature importance\nnot available\nfor this model',
                   ha='center', va='center', transform=axes[1, 1].transAxes, fontsize=12)
    axes[1, 1].set_title('🔍 Feature Importance')

plt.tight_layout()
plt.show()

# Detailed classification report for best model
print(f"\n📋 DETAILED CLASSIFICATION REPORT - {best_model_name}")
print("="*60)
print(classification_report(y_test, best_predictions,
                          target_names=['Normal', 'Medium', 'High', 'Critical']))

# ================================
# CELL 8: Hyperparameter Tuning for Best Model
# ================================
print(f"\n⚙️ HYPERPARAMETER TUNING - {best_model_name}")
print("="*50)

# Define parameter grids
param_grids = {
    'Random Forest': {
        'n_estimators': [50, 100, 200],
        'max_depth': [10, 20, None],
        'min_samples_split': [2, 5, 10]
    },
    'Gradient Boosting': {
        'n_estimators': [50, 100, 200],
        'learning_rate': [0.05, 0.1, 0.2],
        'max_depth': [3, 5, 7]
    },
    'SVM': {
        'C': [0.1, 1, 10],
        'kernel': ['rbf', 'linear'],
        'gamma': ['scale', 'auto']
    },
    'Logistic Regression': {
        'C': [0.1, 1, 10],
        'solver': ['liblinear', 'lbfgs'],
        'max_iter': [1000, 2000]
    }
}

if best_model_name in param_grids:
    print(f"🔄 Tuning {best_model_name}...")

    base_model = models[best_model_name]
    param_grid = param_grids[best_model_name]

    # Use scaled data for appropriate models
    if best_model_name in ['SVM', 'Logistic Regression']:
        grid_search = GridSearchCV(base_model, param_grid, cv=3, scoring='accuracy', n_jobs=-1)
        grid_search.fit(X_train_scaled, y_train)
        tuned_predictions = grid_search.predict(X_test_scaled)
    else:
        grid_search = GridSearchCV(base_model, param_grid, cv=3, scoring='accuracy', n_jobs=-1)
        grid_search.fit(X_train, y_train)
        tuned_predictions = grid_search.predict(X_test)

    tuned_accuracy = accuracy_score(y_test, tuned_predictions)

    print(f"✅ Original Accuracy: {results[best_model_name]['accuracy']:.4f}")
    print(f"🎯 Tuned Accuracy: {tuned_accuracy:.4f}")
    print(f"📈 Improvement: {tuned_accuracy - results[best_model_name]['accuracy']:.4f}")
    print(f"⚙️ Best Parameters: {grid_search.best_params_}")

    # Store the best model
    best_tuned_model = grid_search.best_estimator_
else:
    print(f"⚠️ Hyperparameter tuning not configured for {best_model_name}")
    best_tuned_model = model_objects[best_model_name]

# ================================
# CELL 9: Real-time Threat Prediction Function
# ================================
def predict_threat_realtime(source_ip, attack_type, connection_count, payload_size,
                          session_duration, destination_port, failed_attempts,
                          source_country, hour_of_day, model=best_tuned_model):
    """
    Real-time threat prediction function
    """
    # Create input data
    input_data = {
        'source_ip': source_ip,
        'attack_type': attack_type,
        'connection_count': connection_count,
        'payload_size': payload_size,
        'session_duration': session_duration,
        'destination_port': destination_port,
        'failed_login_attempts': failed_attempts,
        'source_country': source_country,
        'hour_of_day': hour_of_day,
        'day_of_week': 1,  # Default Monday
        'packets_sent': connection_count * 10,  # Estimate
        'bytes_transferred': payload_size * connection_count,
        'unique_protocols': 1
    }

    # Convert to DataFrame
    input_df = pd.DataFrame([input_data])

    # Apply feature engineering
    input_engineered = feature_engineering(input_df)

    # Prepare features (using the same encoders)
    try:
        input_engineered['attack_type_encoded'] = encoders[0].transform([attack_type])[0]
        input_engineered['country_encoded'] = encoders[1].transform([source_country])[0]
        input_engineered['ip_class_encoded'] = encoders[2].transform([source_ip.split('.')[0]])[0]
    except:
        # Handle unseen categories
        input_engineered['attack_type_encoded'] = 0
        input_engineered['country_encoded'] = 0
        input_engineered['ip_class_encoded'] = 0

    # Select features
    X_input = input_engineered[feature_names]

    # Scale if necessary
    if best_model_name in ['SVM', 'K-Neighbors', 'Logistic Regression']:
        X_input_scaled = scaler.transform(X_input)
        prediction = model.predict(X_input_scaled)[0]
        probability = model.predict_proba(X_input_scaled)[0]
    else:
        prediction = model.predict(X_input)[0]
        probability = model.predict_proba(X_input)[0]

    threat_levels = ['Normal', 'Medium', 'High', 'Critical']

    return {
        'threat_level': threat_levels[prediction],
        'threat_level_numeric': prediction,
        'probabilities': {threat_levels[i]: prob for i, prob in enumerate(probability)},
        'confidence': max(probability),
        'risk_assessment': 'HIGH RISK' if prediction >= 2 else 'MEDIUM RISK' if prediction == 1 else 'LOW RISK'
    }

print("🚀 REAL-TIME THREAT PREDICTION SYSTEM READY!")
print("="*50)

# Test the prediction system
test_cases = [
    {
        'source_ip': '192.168.1.100',
        'attack_type': 'NORMAL_TRAFFIC',
        'connection_count': 5,
        'payload_size': 100,
        'session_duration': 60,
        'destination_port': 80,
        'failed_attempts': 0,
        'source_country': 'US',
        'hour_of_day': 14
    },
    {
        'source_ip': '223.5.5.5',
        'attack_type': 'SSH_BRUTEFORCE',
        'connection_count': 200,
        'payload_size': 50,
        'session_duration': 5,
        'destination_port': 22,
        'failed_attempts': 150,
        'source_country': 'CN',
        'hour_of_day': 3
    },
    {
        'source_ip': '117.25.45.67',
        'attack_type': 'WEB_ATTACK',
        'connection_count': 50,
        'payload_size': 1500,
        'session_duration': 30,
        'destination_port': 443,
        'failed_attempts': 10,
        'source_country': 'RU',
        'hour_of_day': 22
    }
]

print("\n🧪 TESTING PREDICTION SYSTEM:")
print("="*50)

for i, test_case in enumerate(test_cases, 1):
    print(f"\n📋 Test Case {i}:")
    for key, value in test_case.items():
        print(f"   {key}: {value}")

    result = predict_threat_realtime(**test_case)

    print(f"\n🎯 PREDICTION RESULTS:")
    print(f"   Threat Level: {result['threat_level']} (Level {result['threat_level_numeric']})")
    print(f"   Risk Assessment: {result['risk_assessment']}")
    print(f"   Confidence: {result['confidence']:.3f}")
    print(f"   Probabilities:")
    for level, prob in result['probabilities'].items():
        print(f"      {level}: {prob:.3f}")
    print("-" * 30)

# ================================
# CELL 10: Model Persistence & Summary
# ================================
import pickle
import json

print("\n💾 SAVING MODEL AND COMPONENTS")
print("="*50)

# Save the trained model
model_filename = f"honeypot_threat_model_{best_model_name.lower().replace(' ', '_')}.pkl"
with open(model_filename, 'wb') as f:
    pickle.dump(best_tuned_model, f)

# Save the scaler
scaler_filename = "honeypot_scaler.pkl"
with open(scaler_filename, 'wb') as f:
    pickle.dump(scaler, f)

# Save encoders
encoders_filename = "honeypot_encoders.pkl"
with open(encoders_filename, 'wb') as f:
    pickle.dump(encoders, f)

# Save feature names
feature_names_filename = "honeypot_features.json"
with open(feature_names_filename, 'w') as f:
    json.dump(feature_names, f)

# Save model metadata
metadata = {
    'best_model': best_model_name,
    'accuracy': float(results_df.iloc[0]['Accuracy']),
    'cv_score': float(results_df.iloc[0]['CV Mean']),
    'feature_count': len(feature_names),
    'training_samples': len(X_train),
    'test_samples': len(X_test),
    'threat_levels': ['Normal', 'Medium', 'High', 'Critical'],
    'creation_date': pd.Timestamp.now().isoformat()
}

metadata_filename = "honeypot_model_metadata.json"
with open(metadata_filename, 'w') as f:
    json.dump(metadata, f, indent=2)

print(f"✅ Model saved as: {model_filename}")
print(f"✅ Scaler saved as: {scaler_filename}")
print(f"✅ Encoders saved as: {encoders_filename}")
print(f"✅ Features saved as: {feature_names_filename}")
print(f"✅ Metadata saved as: {metadata_filename}")

# Display final feature importance if available
if hasattr(best_tuned_model, 'feature_importances_'):
    print("\n🔍 TOP 10 MOST IMPORTANT FEATURES:")
    feature_imp_df = pd.DataFrame({
        'Feature': feature_names,
        'Importance': best_tuned_model.feature_importances_
    }).sort_values('Importance', ascending=False).head(10)

    for idx, row in feature_imp_df.iterrows():
        print(f"   {row['Feature']}: {row['Importance']:.4f}")

print("\n🎉 HONEYPOT THREAT PREDICTION ML SYSTEM COMPLETE!")
print("="*60)
print("📊 FINAL SUMMARY:")
print(f"   🏆 Best Model: {best_model_name}")
print(f"   🎯 Accuracy: {results_df.iloc[0]['Accuracy']:.4f}")
print(f"   📈 CV Score: {results_df.iloc[0]['CV Mean']:.4f} ± {results_df.iloc[0]['CV Std']:.4f}")
print(f"   🔍 Features Used: {len(feature_names)}")
print(f"   📋 Training Samples: {len(X_train):,}")
print(f"   🧪 Test Samples: {len(X_test):,}")
print("="*60)